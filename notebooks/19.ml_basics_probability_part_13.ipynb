{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "plt.xkcd()\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "## Analogy\n",
    "\n",
    "## Diagram\n",
    "\n",
    "## Example\n",
    "\n",
    "## Plain English\n",
    "\n",
    "Regression models allow us to model the relationship (e.g. predict the value of one based on the value of other(s)) between two or more quantities of interested based on data.\n",
    "\n",
    "## Technical Definition\n",
    "\n",
    "The linear regression methodoly can be be entirely explained and developed as least squares approximation procedure, without any probabilistic assumptions.\n",
    "\n",
    "### Simple Linear Regression\n",
    "\n",
    "Let's first consider the case of only two variables. We wish to model the relation between $x$ and $y$, based on collection of data pairs $(x_i, y_i), i = 1,\\ldots,n$. Then our linear model has the form:\n",
    "\n",
    "$$\\hat{y} = aX + b$$\n",
    "\n",
    "where $a$ and $b$ are unknown paramaters to be estimated.\n",
    "\n",
    "Generally, $\\hat{y_i}$ will be different from the given value $y_i$ and the corresponding difference\n",
    "\n",
    "$$\\tilde{y_i} = y_i - \\hat{y_i}$$\n",
    "\n",
    "is called the *i*th **residual**. A choice of parameters that results in small residuals is considered to provide a good fit to the data. Our model tries to choose the parameters $a$ and $b$ so that the sum of squared residuals is minimized:\n",
    "\n",
    "$$\\sum_{i=1}^n(y_i - \\hat{y_i})^2=\\sum_{i=1}^n(y_i - aX_i - b)^2$$\n",
    "\n",
    "over all $a$ and $b$.\n",
    "\n",
    "Note that the linear least squares approach tries to find the best linear model, and involves an implicit hypothesis that the model is valid. However, the true relationship between the variables might not be linear.\n",
    "\n",
    "To obtain the estimates for $a$ and $b$, we observe that given data the sum of the squared residuals is a quadratic function of $a$ and $b$. More formally we get:\n",
    "\n",
    "Given $n$ data pairs $(x_i, y_i)$, the estimates that minimize the sum of the squared residuals are given by\n",
    "\n",
    "$$a = \\rho\\frac{\\sigma{y}}{\\sigma{x}}, \\qquad b=\\overline{y} - a\\overline{x}$$\n",
    "\n",
    "#### Coefficient of determination\n",
    "\n",
    "The coefficient of determination, denoted by $r^2$ is interpreted as the proportion of one variance that is predicted from another variables.\n",
    "\n",
    "- The coefficient of determination is the square of the correlation (r) between predicted y scores and actual y scores; thus, it ranges from 0 to 1\n",
    "- With linear regression, the coefficient of determination is also equal to the square of the correlation between x and y scores\n",
    "- An $r^2$ of 0 means that our model has not predictive power\n",
    "- An $r^2$ of 1 means that the regression line passes through every data point\n",
    "- An $r^2$ between 0 and 1 indicates how well the regression line represents the data\n",
    "\n",
    "The coefficient of determination for a linear regression model with one independent variable is:\n",
    "\n",
    "$$r^2 = 1 - \\frac{\\sum (\\tilde{y_i})^2}{\\sum (y_i - \\overline{y}) ^ 2}$$\n",
    "\n",
    "that is one minus the ratio of sum of squared residuals (SSR) to total sum squared (TSS).\n",
    "\n",
    "### Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/regression-library/v/introduction-to-residuals-and-least-squares-regression\n",
    "- https://onlinecourses.science.psu.edu/stat501/node/250\n",
    "- http://onlinestatbook.com/2/regression/intro.html\n",
    "- http://ci.columbia.edu/ci/premba_test/c0331/s7/s7_6.html\n",
    "- https://www.probabilitycourse.com/chapter8/8_5_0_linear_regression.php"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
