{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "plt.xkcd()\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "\n",
    "## Analogy\n",
    "\n",
    "## Diagram\n",
    "\n",
    "## Example\n",
    "\n",
    "## Plain English\n",
    "\n",
    "The correlation gives us a way to measure how strong the relationship between two variables is.\n",
    "\n",
    "## Technical Definition\n",
    "\n",
    "### Covariance\n",
    "\n",
    "Consider two random variables $X$ and $Y$. The *covariance* between $X$ and $Y$ is written as $Cov(X, Y)$. The covariance gives us information about how $X$ and $Y$ are statistically related. Here is a definition:\n",
    "\n",
    "$$\\text{Cov}(X,Y)=\\mathbb{E}\\big[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])\\big]=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y].$$\n",
    "\n",
    "Proof:\n",
    "\n",
    "Let $a = \\mathbb{E}[X]$ and $b = \\mathbb{E}[Y]$, then we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}\\big[(X - a)(Y - b)\\big] &=\\mathbb{E}(XY - bX - aY + ab)\\\\\n",
    "&=\\mathbb{E}(XY) - b\\mathbb{E}(X) - a\\mathbb{E}(Y) + ab\\\\\n",
    "&=\\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y) - \\mathbb{E}(Y)\\mathbb{E}(X) + \\mathbb{E}(X)\\mathbb{E}(Y)\\\\\n",
    "&=\\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y).\n",
    "\\end{align}$$\n",
    "\n",
    "When $Cov(X, Y) = 0$, we say that $X$ and $Y$ are *uncorrelated*. A positive or negative covariance indicates the direction and strength of the correlation.\n",
    "\n",
    "#### Some properties of covariance\n",
    "\n",
    "For any random variables $X, Y and Z$ and any scalar values $a and b$:\n",
    "\n",
    "1. $Cov(X, X) = var(X)$ \n",
    "2. if $X$ and $Y$ are independent then $Cov(X, Y) = 0$\n",
    "3. $Cov(X, Y) = Cov(Y, X)$\n",
    "4. $Cov(aX,Y)=aCov(X,Y)$\n",
    "5. $Cov(X + c, Y) = Cov(X, Y)$\n",
    "6. $Cov(X + Y, Z) = Cov(X, Z) + Cov(Y, Z)$\n",
    "7. $cov(X, aY + b) = aCov(X, Y)$\n",
    "8. More generally,\n",
    "\n",
    "$$Cov\\left(\\sum_{i=1}^{m}a_iX_i, \\sum_{j=1}^{n}b_jY_j\\right)=\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_ib_j Cov(X_i,Y_j).$$\n",
    "\n",
    "Note that if $X$ and $Y$ are independent, we have $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$, which implies that $Cov(X, Y) = 0.$ Thus, if $X$ and $Y$ are independent, they are uncorrelated. Generally, the converse is not true.\n",
    "\n",
    "### Variance of a sum\n",
    "\n",
    "One common application of covariance is finding the variance of a sum of several random variables. Let $Z = X + Y$, then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Var(Z)&=Cov(Z,Z)\\\\\n",
    "&=Cov(X+Y,X+Y)\\\\\n",
    "&=Cov(X,X)+Cov(X,Y)+ Cov(Y,X)+Cov(Y,Y)\\\\\n",
    "&=Var(X)+Var(Y)+2 Cov(X,Y).\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "### Correlation coefficient\n",
    "\n",
    "The **correlation coefficient** $\\rho(X, Y)$ of two random variables $X$ and $Y$ that have nonzero variances is defined as\n",
    "\n",
    "$$\\rho(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{Var(X)Var(Y)}}$$\n",
    "\n",
    "It can be viewed as normalized version of the covariance $Cov(X, Y)$ and it can be shown that $\\rho$ ranges from -1 to 1.\n",
    "\n",
    "If $\\rho > 0$ or $\\rho < 0$, the the values of $Cov(X, Y)$ \"tend\" to have the same sign. The size of $|\\rho|$ provides a normalized measure of the extent to which this is true. If $\\rho = 1$ or $\\rho = -1$ and assuming that $X$ and $Y$ have positive variances, there exists a positive (or negative, respectively) constant $c$ such that:\n",
    "\n",
    "$$Y - \\mathbb{E}[Y] = c(X - \\mathbb{E}[X])$$\n",
    "\n",
    "#### Some properties of the correlation coefficient\n",
    "\n",
    "For any random variables $X$ and $Y$ and any scalar values $a, b, c and d$:\n",
    "\n",
    "1. $-1 \\leq \\rho(X, Y) \\leq 1$\n",
    "2. if $\\rho(X, Y) = 1$, then $Y = aX + b$, where $a > 0$\n",
    "3. if $\\rho(X, Y) = -1$, then $Y = aX + b$, where $a < 0$\n",
    "4. $\\rho(aX + b, cY + d) = \\rho(X,Y)$ for $a, c > 0$\n",
    "\n",
    "Note that if $X$ and $Y$ are uncorrelated, we can conclude that \n",
    "\n",
    "$$Var(X + Y) = Var(X) + Var(Y).$$\n",
    "\n",
    "In practice, we often talk about the following degrees of correlation:\n",
    "\n",
    "- weak - $0 \\leq \\rho < 0.3$\n",
    "- moderate - $0.3 \\leq \\rho < 0.5$\n",
    "- significant - $0.5 \\leq \\rho < 0.7$\n",
    "- strong - $0.7 \\leq \\rho < 0.9$\n",
    "- very strong - $0.9 \\leq \\rho$\n",
    "\n",
    "The degree of correlation can also be described using the Pearson coefficient (in the case that we have discrete two dimensional space) which is defined as:\n",
    "\n",
    "$$\\phi^2 = \\sum_{i, j}\\frac{(p_{ij} - p_ip_j)^2}{p_ip_j}$$ given that $p_i = \\sum_j p_{ij}$ and $p_j = \\sum_i p{ij}$. If the random variables $X$ and $Y$ are independent $phi^2 = 0$. The reverse is also true."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
