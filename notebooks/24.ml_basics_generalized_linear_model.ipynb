{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обобщен линеен модел (Generalized Linear Model)\n",
    "\n",
    "Обобщените линейни модели (ОЛМ) разширяват рамката за линейно моделиране за променливи, които не са нормално разпределени. В практиката, ОЛМ се използват за моделиране на двоични или категорийни данни.\n",
    "\n",
    "## Общ линеен модел (General Linear Model)\n",
    "\n",
    "В общият си вид този модел може да се представи като:\n",
    "\n",
    "$$y_i = b_0 + b_1x_{1i} + \\ldots + b_px_{pi} + \\epsilon_i$$\n",
    "\n",
    "където зависимата променлива $y_i, i=1, \\ldots, n$ е моделирана като линейна функция от независимите променливи $x_j, j=1, \\ldots, p$ и $epsilon_i, i=1, \\ldots, n$ е грешката. Наблюденията $y$ са независими реализации на (едномерна) сл.в. $Y$ с разпределение, чиято плътност е $f(y;\\theta,\\phi)$, като параметрите $\\theta$ и $\\phi$ са неизвестни.\n",
    "\n",
    "Тук, **общ** се отнася за възможното използване на повече от една независима променлива, за разлика от простия линеен модел за който имаме:\n",
    "\n",
    "$$y_i = b_0 + b_1x_i+e_i$$\n",
    "\n",
    "### Структура на грешките\n",
    "\n",
    "Предполагаме, че грешките $\\epsilon_i$ са независими и идентично разпределени така че:\n",
    "\n",
    "$$\n",
    "E[\\epsilon_i] = 0 \\\\\n",
    "\\text{and var}[\\epsilon_i] = \\sigma^2 \n",
    "$$\n",
    "\n",
    "и са нормално разпределени:\n",
    "\n",
    "$$\\epsilon_i \\sim N(0, \\sigma^2)$$\n",
    "\n",
    "### Множествена линейна регресия\n",
    "\n",
    "Множествената линейна регресия е обобщение на простата линейна регресия, като се вземат под предвид повече от една независима променлива. Нека отново имаме разглежданият модел:\n",
    "\n",
    "$$y = Xb + e$$\n",
    "\n",
    "**Теорема** Ако $X$ има пълен ранг $m$, оценката на неизвестните параметри $b$ по метода на най-малките квадрати е\n",
    "\n",
    "$$\\hat{e} = (X'X)^{-1}X'y$$\n",
    "$$\\text{cov}(\\hat{e}) = \\sigma^2(X'X)^{-1}$$\n",
    "\n",
    "Оценката $\\hat{e}$ е неизместена, ефективна и съвпада с оценката по метода на максимално правдоподобие.\n",
    "\n",
    "### Ограничения на линейните модели\n",
    "\n",
    "Въпреки полезната рамка, която предлагат общите линейни модели, има ситуации в които те не са подходящи за употреба:\n",
    "\n",
    "- Областта на $Y$ е ограничена (двоични или категорийни данни)\n",
    "- Дисперсията на $Y$ зависи от средната стойност\n",
    "\n",
    "Обобщените линейни модели разширяват рамката, като решават и двата проблема.\n",
    "\n",
    "## Обобщен линеен модел\n",
    "\n",
    "ОЛМ са съставени от линеен предиктор:\n",
    "\n",
    "$$\\eta_i=b_0 + b_1x_{1i} + \\ldots + b_px_{pi}$$\n",
    "\n",
    "и две функции\n",
    "\n",
    "- **свързваща** функция, която описва как средната стойност, $E(Y_i) = \\mu_i$, зависи от линейния предиктор\n",
    "\n",
    "$$g(\\mu_i) = \\eta_i$$\n",
    "\n",
    "Тя обикновено е обратима, т.е. съществува обратната функция $\\mu = g^{-1}(\\eta)$, която се нарича функция на средната.\n",
    "\n",
    "- функция на **дисперсията**, която описва как дисперсията, $\\text{var}(Y_i)$, зависи от средната стойност:\n",
    "\n",
    "$$\\text{var}(Y_i)=\\phi V(\\mu)$$\n",
    "\n",
    "където дисперсионният параметър $\\phi$ е константа.\n",
    "\n",
    "### Нормален обобщен линеен модел\n",
    "\n",
    "За обобщен линеен модел с грешка $\\epsilon_i \\sim N(0, \\sigma^2)$ имаме линеен предиктор:\n",
    "\n",
    "$$\\eta_i=b_0 + b_1x_{1i} + \\ldots + b_px_{pi}$$\n",
    "\n",
    "свързваща функция\n",
    "\n",
    "$$g(\\mu_i) = \\mu_i$$\n",
    "\n",
    "и функция на дисперсията:\n",
    "\n",
    "$$\\text{var}(\\mu_i)=1$$\n",
    "\n",
    "### Оценка на параметрите\n",
    "\n",
    "Когато плътността $f(y;\\theta,\\phi)$ принадлежи на експоненциалната фамилия, ФП има единствен максимум и оценките на параметрите могат да се определят еднозначно. Тук попадат много статистически процедури - линейна регресия, логистична регресия, като те се различават по свързващата функция и плътността на зависимата променлива. Най-често използваният метод за оценка на параметри в ОЛМ е Итеративен претеглен метод на най-малките квадрати !? (Iteratively Weighted Least Squares).\n",
    "\n",
    "## Бейсов подход за ОЛМ\n",
    "\n",
    "Класическият подход предполага, че оценката на параметрите се свежда до максимизиране (оптимизация) на ФП. Например, за простата линейна регресия имаме:\n",
    "\n",
    "$$\\DeclareMathOperator*{\\argmax}{arg\\,max} \\argmax_{\\alpha,\\,\\beta,\\,\\sigma} \\prod_{i=1}^n \\mathcal{N}(y_i; \\alpha + \\beta x_i, \\sigma)$$\n",
    "\n",
    "където $\\mathcal{N}$ e плътностна функция на нормално разпределение в точки $y_i$, параметризирана от средните $\\alpha + \\beta x_i$ и стандартно отклонение $\\sigma$.\n",
    "\n",
    "Бейсовият подход за ОЛМ добавя априорно разпределение за параметрите, вместо максимизиране на ФП. За простата линейна регресия имаме:\n",
    "\n",
    "$$\\underbrace{f(\\alpha,\\beta,\\sigma\\mid Y,X)}_{\\text{posterior}} \\propto \\underbrace{\\prod_{i=1}^n \\mathcal{N}(y_i\\mid \\alpha + \\beta x_i, \\sigma)}_{\\text{likelihood}} \\; \\underbrace{f_{\\alpha}(\\alpha) \\, f_{\\beta}(\\beta) \\, f_{\\sigma}(\\sigma)}_{\\text{priors}}$$\n",
    "\n",
    "ФП е същата като тази отгоре, но имаме априорно разпределение за параметрите, които трябва да оценим $\\alpha,\\beta,\\sigma$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
