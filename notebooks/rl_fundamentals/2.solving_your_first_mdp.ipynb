{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around you will solve the MDP defined in the last part. You will get that ice cream! First, we need a bit more learnings:\n",
    "\n",
    "# Discounted Future Reward\n",
    "\n",
    "Why do we need the discount factor $\\gamma$? The total reward that our agent will receive from the current time step t to the end of the task can be defined as:\n",
    "\n",
    "$$R_t = r_t + r_{t + 1} + \\ldots + r_n$$\n",
    "\n",
    "That looks ok, but let's not forget that our environment is stochastic (the supermarket might close any time now). The discount factor allows us to value short-term reward more than long-term ones, we can use it as:\n",
    "\n",
    "$$R_t = R_t + \\gamma r_{t+1} + \\ldots + \\gamma^{n - t} r_n$$\n",
    "\n",
    "Our agent would perform great if he chooses the action that maximizes the (discounted) future reward at every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function\n",
    "\n",
    "It would be great to know how \"good\" a given state $s$ is. Something to tell us - no matter the state you're in if you transition to state $s$ your total reward will be $x$, word! If you start from $s$ and follow policy $\\pi$. That would spare us from revisiting same states over and over again. The **value function** does this for us. It depends on the state we're in $s$ and the policy $\\pi$ our agent is following. It is given by:\n",
    "\n",
    "$$V^{\\pi}(s) = \\mathbb{E}(\\sum_{t \\geq 0}\\gamma^t r_t) \\quad \\forall s \\in \\mathbb{S}$$\n",
    "\n",
    "There exists an **optimal value function** that has the highest value for all states. It is given by:\n",
    "\n",
    "$$V^*(s) = \\max_{\\pi}V^{\\pi}(s) \\quad \\forall s \\in \\mathbb{S}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q function\n",
    "\n",
    "Yet, your agent can't control what state he ends up in, directly. He can influence it by choosing some action $a$.  Let's introduce another function that accepts state and action as parameters and returns the expected total reward - the Q function. More formally, the function $Q^{\\pi}(s, a)$ gives the expected return when starting in $s$, performing $a$ and following $\\pi$.\n",
    "\n",
    "Again, we can define the optimal Q-function $Q^*(s, a)$ that gives the expected total reward for our agent when starting at $s$ and picks action $a$. That is, the optimal Q-function tells our agent how good of a choice is picking $a$ when at state $s$.\n",
    "\n",
    "There is a relationship between the two optimal functions $V^*$ and $Q^*$. Since $V^*$ is the highest expected total reward when starting at $s$, it will be "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
