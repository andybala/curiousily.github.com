{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around you will solve the MDP defined in the last part. You will get that ice cream! But first, we need a bit more learnings:\n",
    "\n",
    "# Discounted Future Reward\n",
    "\n",
    "Why do we need the discount factor $\\gamma$? The total reward that our agent will receive from the current time step t to the end of the task can be defined as:\n",
    "\n",
    "$$R_t = r_t + r_{t + 1} + \\ldots + r_n$$\n",
    "\n",
    "That looks ok, but let's not forget that our environment is stochastic (the supermarket might close any time now). The discount factor allows us to value short-term reward more than long-term ones, we can use it as:\n",
    "\n",
    "$$R_t = R_t + \\gamma r_{t+1} + \\ldots + \\gamma^{n - t} r_n = r_t + \\gamma R_{t+1}$$\n",
    "\n",
    "Our agent would perform great if he chooses the action that maximizes the (discounted) future reward at every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function\n",
    "\n",
    "It would be great to know how \"good\" a given state $s$ is. Something to tell us - no matter the state you're in if you transition to state $s$ your total reward will be $x$, word! If you start from $s$ and follow policy $\\pi$. That would spare us from revisiting same states over and over again. The **value function** does this for us. It depends on the state we're in $s$ and the policy $\\pi$ our agent is following. It is given by:\n",
    "\n",
    "$$V^{\\pi}(s) = \\mathbb{E}(\\sum_{t \\geq 0}\\gamma^t r_t) \\quad \\forall s \\in \\mathbb{S}$$\n",
    "\n",
    "There exists an **optimal value function** that has the highest value for all states. It is given by:\n",
    "\n",
    "$$V^*(s) = \\max_{\\pi}V^{\\pi}(s) \\quad \\forall s \\in \\mathbb{S}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q function\n",
    "\n",
    "Yet, your agent can't control what state he ends up in, directly. He can influence it by choosing some action $a$.  Let's introduce another function that accepts state and action as parameters and returns the expected total reward - the Q function (it represents the \"quality\" of a certain action given a state). More formally, the function $Q^{\\pi}(s, a)$ gives the expected return when starting in $s$, performing $a$ and following $\\pi$.\n",
    "\n",
    "Again, we can define the optimal Q-function $Q^*(s, a)$ that gives the expected total reward for our agent when starting at $s$ and picks action $a$. That is, the optimal Q-function tells our agent how good of a choice is picking $a$ when at state $s$.\n",
    "\n",
    "There is a relationship between the two optimal functions $V^*$ and $Q^*$. It is given by:\n",
    "\n",
    "$$V^*(s) = \\max_aQ^*(s, a) \\quad \\forall s \\in \\mathbb{S}$$\n",
    "\n",
    "That is, the maximum expected total reward when starting at $s$ is the maximum of $Q^*(s, a)$ over all possible actions.\n",
    "\n",
    "Using $Q^*(s, a)$ we can extract the optimal policy $\\pi^*$ by choosing the action $a$ that gives maximum reward $Q^*(s, a)$ for state $s$. We have:\n",
    "\n",
    "$$\\pi^*(s) = \\text{arg}\\max_{a} Q^* (s, a) \\quad \\forall s \\in \\mathbb{S}$$\n",
    "\n",
    "There is a nice relationship between all functions we defined so far. You now have the tools to identify states and state-action pairs as good or bad. More importantly, if you can identify $V^*$ or $Q^*$, you can build the best possible agent there is (for the current environment). But how do we use this in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with Q-learning\n",
    "\n",
    "Let's focus on a single state $s$ and action $a$. We can express $Q(s, a)$ recursively, in terms of the Q-value of the next state $s'$:\n",
    "\n",
    "$$Q(s, a) = r + \\gamma \\max_{a'}Q(s', a')$$\n",
    "\n",
    "This equation, known as the **Bellman equation**, tells us that the maximum future reward is the reward the agent received for entering the current state $s$ plus the maximum future reward for the next state $s'$. The gist of Q-learning is that we can iteratively approximate $Q^*$ using the Bellman equation described above. The Q-learning equation is given by:\n",
    "\n",
    "$$Q_{t+1}(s_t, a_t) = Q_t(s_t, a_t) + \\alpha(r_{t+1} + \\gamma \\max_{a}Q_t(s_{t + 1}, a) - Q_t(s_t, a_t))$$\n",
    "\n",
    "where $\\alpha$ is the learning rate that controls how much the difference between previous and new Q-value is considered.\n",
    "\n",
    "Can our agent learn anything using this? At first - no, the initial approximations will most likely be completely random/wrong. However, as the agent explore more and more of the environment, the approximated Q-values will start to converge to $Q^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Environment\n",
    "\n",
    "Okay, it is time to get your ice cream. Let's try a simple case first:\n",
    "\n",
    "![](./2.rl_for_hackers_part_1_files/small_grid.png)\n",
    "\n",
    "Let's define our state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZOMBIE = \"z\"\n",
    "CAR = \"c\"\n",
    "ICE_CREAM = \"i\"\n",
    "EMPTY = \"*\"\n",
    "\n",
    "grid = [\n",
    "    [ICE_CREAM, EMPTY],\n",
    "    [ZOMBIE, CAR]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i *\n",
      "z c\n"
     ]
    }
   ],
   "source": [
    "for row in grid:\n",
    "    print(' '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "class State:\n",
    "    \n",
    "    def __init__(self, grid, car_pos):\n",
    "        self.grid = grid\n",
    "        self.car_pos = car_pos\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, State) and self.grid == other.grid and self.car_pos == other.car_pos\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self.grid) + str(self.car_pos))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"State(grid={self.grid}, car_pos={self.car_pos})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All possible actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the initial state and its actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your agent needs a way to interact with the environment, that is, choose its actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def act(state, action):\n",
    "    \n",
    "    def new_car_pos(state, action):\n",
    "        p = deepcopy(state.car_pos)\n",
    "        if action == UP:\n",
    "            p[0] = max(0, p[0] - 1)\n",
    "        elif action == DOWN:\n",
    "            p[0] = min(len(state.grid) - 1, p[0] + 1)\n",
    "        elif action == LEFT:\n",
    "            p[1] = max(0, p[1] - 1)\n",
    "        elif action == RIGHT:\n",
    "            p[1] = min(len(state.grid[0]) - 1, p[1] + 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown action {action}\")\n",
    "        return p\n",
    "            \n",
    "    p = new_car_pos(state, action)\n",
    "    grid_item = state.grid[p[0]][p[1]]\n",
    "    \n",
    "    new_grid = deepcopy(state.grid)\n",
    "    \n",
    "    if grid_item == ZOMBIE:\n",
    "        reward = -100\n",
    "        is_done = True\n",
    "        new_grid[p[0]][p[1]] += CAR\n",
    "    elif grid_item == ICE_CREAM:\n",
    "        reward = 1000\n",
    "        is_done = True\n",
    "        new_grid[p[0]][p[1]] += CAR\n",
    "    elif grid_item == EMPTY:\n",
    "        reward = -1\n",
    "        is_done = False\n",
    "        old = state.car_pos\n",
    "        new_grid[old[0]][old[1]] = EMPTY\n",
    "        new_grid[p[0]][p[1]] = CAR\n",
    "    elif grid_item == CAR:\n",
    "        reward = -1\n",
    "        is_done = False\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown grid item {grid_item}\")\n",
    "    \n",
    "    return State(grid=new_grid, car_pos=p), reward, is_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"State(grid=[['i', '*'], ['z', 'c']], car_pos=[1, 1])\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_state = State(grid, [1, 1])\n",
    "str(start_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_state == State(grid, [1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, r, is_done = act(start_state, LEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_states = 4\n",
    "iter_max = 100\n",
    "\n",
    "gamma = 1.0\n",
    "t_max = 10000\n",
    "eps = 0.2\n",
    "\n",
    "q_table = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1 -- Total reward = 999.\n",
      "Iteration #11 -- Total reward = 999.\n",
      "Iteration #21 -- Total reward = 999.\n",
      "Iteration #31 -- Total reward = 999.\n",
      "Iteration #41 -- Total reward = 999.\n",
      "Iteration #51 -- Total reward = 999.\n",
      "Iteration #61 -- Total reward = 999.\n",
      "Iteration #71 -- Total reward = 999.\n",
      "Iteration #81 -- Total reward = 999.\n",
      "Iteration #91 -- Total reward = 999.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def q(state, action=None):\n",
    "    \n",
    "    if state not in q_table:\n",
    "        q_table[state] = np.zeros(len(ACTIONS))\n",
    "        \n",
    "    if action is None:\n",
    "        return q_table[state]\n",
    "    \n",
    "    return q_table[state][action]\n",
    "\n",
    "for i in range(iter_max):\n",
    "    state = start_state\n",
    "    \n",
    "    total_reward = 0\n",
    "    alpha = 1.0\n",
    "    for j in range(t_max):\n",
    "        action = random.choice(ACTIONS) if random.uniform(0, 1) < eps else np.argmax(q(state))\n",
    "        next_state, reward, done = act(state, action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        q(state)[action] = q(state, action) + \\\n",
    "                alpha * (reward + gamma *  np.max(q(next_state)) - q(state, action))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration #%d -- Total reward = %d.' %(i+1, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 999.,  998., -100.,  998.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(start_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns, r, done = act(start_state, UP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"State(grid=[['i', 'c'], ['z', '*']], car_pos=[0, 1])\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  999.,   998.,  1000.,   999.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(ns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
