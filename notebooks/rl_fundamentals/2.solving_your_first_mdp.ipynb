{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around you will solve the MDP defined in the last part. You will get that ice cream! But first, we need a bit more learnings:\n",
    "\n",
    "# Discounted Future Reward\n",
    "\n",
    "Why do we need the discount factor $\\gamma$? The total reward that our agent will receive from the current time step t to the end of the task can be defined as:\n",
    "\n",
    "$$R_t = r_t + r_{t + 1} + \\ldots + r_n$$\n",
    "\n",
    "That looks ok, but let's not forget that our environment is stochastic (the supermarket might close any time now). The discount factor allows us to value short-term reward more than long-term ones, we can use it as:\n",
    "\n",
    "$$R_t = R_t + \\gamma r_{t+1} + \\ldots + \\gamma^{n - t} r_n = r_t + \\gamma R_{t+1}$$\n",
    "\n",
    "Our agent would perform great if he chooses the action that maximizes the (discounted) future reward at every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function\n",
    "\n",
    "It would be great to know how \"good\" a given state $s$ is. Something to tell us - no matter the state you're in if you transition to state $s$ your total reward will be $x$, word! If you start from $s$ and follow policy $\\pi$. That would spare us from revisiting same states over and over again. The **value function** does this for us. It depends on the state we're in $s$ and the policy $\\pi$ our agent is following. It is given by:\n",
    "\n",
    "$$V^{\\pi}(s) = \\mathbb{E}(\\sum_{t \\geq 0}\\gamma^t r_t) \\quad \\forall s \\in \\mathbb{S}$$\n",
    "\n",
    "There exists an **optimal value function** that has the highest value for all states. It is given by:\n",
    "\n",
    "$$V^*(s) = \\max_{\\pi}V^{\\pi}(s) \\quad \\forall s \\in \\mathbb{S}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q function\n",
    "\n",
    "Yet, your agent can't control what state he ends up in, directly. He can influence it by choosing some action $a$.  Let's introduce another function that accepts state and action as parameters and returns the expected total reward - the Q function (it represents the \"quality\" of a certain action given a state). More formally, the function $Q^{\\pi}(s, a)$ gives the expected return when starting in $s$, performing $a$ and following $\\pi$.\n",
    "\n",
    "Again, we can define the optimal Q-function $Q^*(s, a)$ that gives the expected total reward for our agent when starting at $s$ and picks action $a$. That is, the optimal Q-function tells our agent how good of a choice is picking $a$ when at state $s$.\n",
    "\n",
    "There is a relationship between the two optimal functions $V^*$ and $Q^*$. It is given by:\n",
    "\n",
    "$$V^*(s) = \\max_aQ^*(s, a) \\quad \\forall s \\in \\mathbb{S}$$\n",
    "\n",
    "That is, the maximum expected total reward when starting at $s$ is the maximum of $Q^*(s, a)$ over all possible actions.\n",
    "\n",
    "Using $Q^*(s, a)$ we can extract the optimal policy $\\pi^*$ by choosing the action $a$ that gives maximum reward $Q^*(s, a)$ for state $s$. We have:\n",
    "\n",
    "$$\\pi^*(s) = \\text{arg}\\max_{a} Q^* (s, a) \\quad \\forall s \\in \\mathbb{S}$$\n",
    "\n",
    "There is a nice relationship between all functions we defined so far. You now have the tools to identify states and state-action pairs as good or bad. More importantly, if you can identify $V^*$ or $Q^*$, you can build the best possible agent there is (for the current environment). But how do we use this in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with Q-learning\n",
    "\n",
    "Let's focus on a single state $s$ and action $a$. We can express $Q(s, a)$ recursively, in terms of the Q-value of the next state $s'$:\n",
    "\n",
    "$$Q(s, a) = r + \\gamma \\max_{a'}Q(s', a')$$\n",
    "\n",
    "This equation, known as the **Bellman equation**, tells us that the maximum future reward is the reward the agent received for entering the current state $s$ plus the maximum future reward for the next state $s'$. The gist of Q-learning is that we can iteratively approximate $Q^*$ using the Bellman equation described above. The Q-learning equation is given by:\n",
    "\n",
    "$$Q_{t+1}(s_t, a_t) = Q_t(s_t, a_t) + \\alpha(r_{t+1} + \\gamma \\max_{a}Q_t(s_{t + 1}, a) - Q_t(s_t, a_t))$$\n",
    "\n",
    "where $\\alpha$ is the learning rate that controls how much the difference between previous and new Q-value is considered.\n",
    "\n",
    "Can our agent learn anything using this? At first - no, the initial approximations will most likely be completely random/wrong. However, as the agent explore more and more of the environment, the approximated Q-values will start to converge to $Q^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Environment\n",
    "\n",
    "Okay, it is time to get your ice cream. Let's try a simple case first:\n",
    "\n",
    "![](./2.rl_for_hackers_part_1_files/small_grid.png)\n",
    "\n",
    "Let's define our state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZOMBIE = \"z\"\n",
    "CAR = \"c\"\n",
    "ICE_CREAM = \"i\"\n",
    "EMPTY = \"*\"\n",
    "\n",
    "grid = [\n",
    "    [ICE_CREAM, EMPTY],\n",
    "    [ZOMBIE, CAR]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i *\n",
      "z c\n"
     ]
    }
   ],
   "source": [
    "for row in grid:\n",
    "    print(' '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "class State:\n",
    "    \n",
    "    def __init__(self, grid, car_pos):\n",
    "        self.grid = grid\n",
    "        self.car_pos = car_pos\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, State) and self.grid == other.grid and self.car_pos == other.car_pos\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self.grid) + str(self.car_pos))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"State(grid={self.grid}, car_pos={self.car_pos})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All possible actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the initial state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_state = State(grid=grid, car_pos=[1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your agent needs a way to interact with the environment, that is, choose actions. Let's define a function that takes the current state with an action and returns new state, reward and whether or not the episode has completed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def act(state, action):\n",
    "    \n",
    "    def new_car_pos(state, action):\n",
    "        p = deepcopy(state.car_pos)\n",
    "        if action == UP:\n",
    "            p[0] = max(0, p[0] - 1)\n",
    "        elif action == DOWN:\n",
    "            p[0] = min(len(state.grid) - 1, p[0] + 1)\n",
    "        elif action == LEFT:\n",
    "            p[1] = max(0, p[1] - 1)\n",
    "        elif action == RIGHT:\n",
    "            p[1] = min(len(state.grid[0]) - 1, p[1] + 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown action {action}\")\n",
    "        return p\n",
    "            \n",
    "    p = new_car_pos(state, action)\n",
    "    grid_item = state.grid[p[0]][p[1]]\n",
    "    \n",
    "    new_grid = deepcopy(state.grid)\n",
    "    \n",
    "    if grid_item == ZOMBIE:\n",
    "        reward = -100\n",
    "        is_done = True\n",
    "        new_grid[p[0]][p[1]] += CAR\n",
    "    elif grid_item == ICE_CREAM:\n",
    "        reward = 1000\n",
    "        is_done = True\n",
    "        new_grid[p[0]][p[1]] += CAR\n",
    "    elif grid_item == EMPTY:\n",
    "        reward = -1\n",
    "        is_done = False\n",
    "        old = state.car_pos\n",
    "        new_grid[old[0]][old[1]] = EMPTY\n",
    "        new_grid[p[0]][p[1]] = CAR\n",
    "    elif grid_item == CAR:\n",
    "        reward = -1\n",
    "        is_done = False\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown grid item {grid_item}\")\n",
    "    \n",
    "    return State(grid=new_grid, car_pos=p), reward, is_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, one episode is starting from the initial state and crashing into a Zombie or eating the ice cream.\n",
    "\n",
    "# Learning to drive\n",
    "\n",
    "Ok, it is time to implement the Q-learning algorithm and get the ice cream. We have a really small state space, only 4 states. This allows us to keep things simple and store the computed Q values in a table. Let's start with some constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(42) # for reproducibility\n",
    "\n",
    "N_STATES = 4\n",
    "N_EPISODES = 20\n",
    "\n",
    "MAX_EPISODE_STEPS = 100\n",
    "\n",
    "MIN_ALPHA = 0.02\n",
    "\n",
    "alphas = np.linspace(1.0, MIN_ALPHA, N_EPISODES)\n",
    "gamma = 1.0\n",
    "eps = 0.2\n",
    "\n",
    "q_table = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(state, action=None):\n",
    "    \n",
    "    if state not in q_table:\n",
    "        q_table[state] = np.zeros(len(ACTIONS))\n",
    "        \n",
    "    if action is None:\n",
    "        return q_table[state]\n",
    "    \n",
    "    return q_table[state][action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < eps:\n",
    "        return random.choice(ACTIONS) \n",
    "    else:\n",
    "        return np.argmax(q(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #1 - Total reward = 999.\n",
      "Episode #2 - Total reward = 998.\n",
      "Episode #3 - Total reward = 997.\n",
      "Episode #4 - Total reward = 997.\n",
      "Episode #5 - Total reward = 999.\n",
      "Episode #6 - Total reward = 999.\n",
      "Episode #7 - Total reward = 998.\n",
      "Episode #8 - Total reward = -100.\n",
      "Episode #9 - Total reward = -101.\n",
      "Episode #10 - Total reward = 999.\n",
      "Episode #11 - Total reward = 999.\n",
      "Episode #12 - Total reward = 999.\n",
      "Episode #13 - Total reward = 999.\n",
      "Episode #14 - Total reward = 999.\n",
      "Episode #15 - Total reward = 999.\n",
      "Episode #16 - Total reward = 998.\n",
      "Episode #17 - Total reward = 999.\n",
      "Episode #18 - Total reward = 999.\n",
      "Episode #19 - Total reward = 999.\n",
      "Episode #20 - Total reward = 999.\n"
     ]
    }
   ],
   "source": [
    "for e in range(N_EPISODES):\n",
    "    \n",
    "    state = start_state\n",
    "    total_reward = 0\n",
    "    alpha = alphas[e]\n",
    "    \n",
    "    for _ in range(MAX_EPISODE_STEPS):\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done = act(state, action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        q(state)[action] = q(state, action) + \\\n",
    "                alpha * (reward + gamma *  np.max(q(next_state)) - q(state, action))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    print('Episode #%d - Total reward = %d.' %(e+1, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 998.9999565 ,  225.12936017,  -85.10182825,  586.19245204])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(start_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns, r, done = act(start_state, UP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"State(grid=[['i', 'c'], ['z', '*']], car_pos=[0, 1])\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  999.,   998.,  1000.,   999.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't this amazing? Our agent doesn't know anything about the \"rules of the game\", yet it manages to learn that Zombies are bad and Ice Cream is great! Also, it tries to reach the ice cream as quickly as possible. The reward is the ultimate signal that drives the learning process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
